{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46c8cef-7815-49c1-9d17-28e8c3d6a3bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# File location and type\n",
    "file_location = \"dbfs:/mnt/olist-silver/order_reviews_fortrans\"\n",
    "file_type = \"Delta\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "source_table = 'default.reviews_to_translate_copy'\n",
    "\n",
    "# Your destination table (output table) catalog.schema.destination_table\n",
    "destination_table = 'default.reviews_to_translate_copy_result'\n",
    "\n",
    "spark.sql(f\"drop table if exists {source_table}\")\n",
    "spark.sql(f\"drop table if exists {destination_table}\")\n",
    "\n",
    "if not spark.catalog.tableExists(source_table):\n",
    "  df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(source_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8af5d5c-a3a5-4fad-9e44-5896f0220c8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(destination_table):\n",
    "    source_df = spark.read.table(source_table)\n",
    "    source_df.write.format(\"delta\").mode(\"append\").saveAsTable(destination_table)\n",
    "    spark.sql(f\"alter table {destination_table} add column status_code int\")\n",
    "    spark.sql(f\"alter table {destination_table} add column processed boolean\")\n",
    "    spark.sql(f\"UPDATE {destination_table} SET processed = 0\")\n",
    "    spark.sql(f\"alter table {destination_table} add column processed_at timestamp\")\n",
    "    spark.sql(\n",
    "        f\"alter table {destination_table} add column review_comment_title_translated string\")\n",
    "    spark.sql(\n",
    "        f\"alter table {destination_table} add column review_comment_message_tr_translated string\")\n",
    "    spark.sql(f\"alter table {destination_table} add column error string\")\n",
    "    destination_delta_table = DeltaTable.forName(spark, destination_table)\n",
    "else:\n",
    "    destination_delta_table = DeltaTable.forName(spark, destination_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e10e17-cf0c-47ef-ad8e-fff29c1e69dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests, uuid, json\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, struct, lit\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9578aee2-3c15-4540-85b9-f6a138586ce4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check if the given string is null, empty, or effectively null.\n",
    "def is_null_or_empty(input_str: str):\n",
    "    return not input_str or not input_str.strip() or not input_str.lower() != \"null\"\n",
    "\n",
    "#Searches for the first occurrence of a specified element within a list and returns its index.If the element is not found, returns -1\n",
    "def find_index_in_list(input_list, command_type):\n",
    "    if command_type in input_list:\n",
    "        return input_list.index(command_type)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#The function attempts to translate the 'review_comment_title' and 'review_comment_message' fields from the input dictionary,handling API requests and responses using the Azure Translator API.\n",
    "\n",
    "def process_data(row):\n",
    "\n",
    "    # Add your key and endpoint\n",
    "    key = \"f3960753d1b948ddb7c2599a912c9a85\"\n",
    "    endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
    "    location = \"southeastasia\"\n",
    "    path = '/translate'\n",
    "    constructed_url = endpoint + path\n",
    "\n",
    "    params = {\n",
    "        'api-version': '3.0',\n",
    "        'from': 'pt',\n",
    "        'to': ['en']\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': key,\n",
    "        # location required if you're using a multi-service or regional (not global) resource.\n",
    "        'Ocp-Apim-Subscription-Region': location,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    tracker = []\n",
    "    body = []\n",
    "    if not is_null_or_empty(row['review_comment_title']):\n",
    "        tracker.append('review_comment_title')\n",
    "        body.append({\n",
    "            'text': row['review_comment_title']\n",
    "        })\n",
    "    if not is_null_or_empty(row['review_comment_message']):\n",
    "        tracker.append('review_comment_message')\n",
    "        body.append({\n",
    "            'text': row['review_comment_message']\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        response = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        response_content = json.loads(response.text)\n",
    "        review_comment_title_translated = None\n",
    "        review_comment_title_index = find_index_in_list(tracker, 'review_comment_title')\n",
    "        review_comment_message_tr_translated = None\n",
    "        review_comment_message_tr_index = find_index_in_list(tracker, 'review_comment_message')\n",
    "        if review_comment_title_index != -1:\n",
    "            review_comment_title_translated = response_content[review_comment_title_index]['translations'][0]['text']\n",
    "        if review_comment_message_tr_index != -1:\n",
    "            review_comment_message_tr_translated = response_content[review_comment_message_tr_index]['translations'][0]['text']\n",
    "\n",
    "        return {\"status_code\": response.status_code if 'response' in locals() else None,\n",
    "                \"processed_at\": datetime.now(timezone.utc),\n",
    "                \"review_comment_title_translated\": review_comment_title_translated,\n",
    "                \"review_comment_message_tr_translated\": review_comment_message_tr_translated, \"error\": None}\n",
    "    except Exception as e:\n",
    "        return {\"status_code\": response.status_code if 'response' in locals() else None,\n",
    "                \"processed_at\": datetime.now(timezone.utc), \"review_comment_title_translated\": None,\n",
    "                \"review_comment_message_tr_translated\": None, \"error\": str(e)}\n",
    "    if not is_null_or_empty(row['review_comment_title']):\n",
    "        tracker.append('review_comment_title')\n",
    "        body.append({\n",
    "            'text': row['review_comment_title']\n",
    "        })\n",
    "    if not is_null_or_empty(row['review_comment_message']):\n",
    "        tracker.append('review_comment_message')\n",
    "        body.append({\n",
    "            'text': row['review_comment_message']\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        response = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        response_content = json.loads(response.text)\n",
    "        review_comment_title_translated = None\n",
    "        review_comment_title_index = find_index_in_list(tracker, 'review_comment_title')\n",
    "        review_comment_message_tr_translated = None\n",
    "        review_comment_message_tr_index = find_index_in_list(tracker, 'review_comment_message')\n",
    "        if review_comment_title_index != -1:\n",
    "            review_comment_title_translated = response_content[review_comment_title_index]['translations'][0]['text']\n",
    "        if review_comment_message_tr_index != -1:\n",
    "            review_comment_message_tr_translated = response_content[review_comment_message_tr_index]['translations'][0][\n",
    "                'text']\n",
    "\n",
    "        return {\"status_code\": response.status_code if 'response' in locals() else None,\n",
    "                \"processed_at\": datetime.now(timezone.utc),\n",
    "                \"review_comment_title_translated\": review_comment_title_translated,\n",
    "                \"review_comment_message_tr_translated\": review_comment_message_tr_translated, \"error\": None}\n",
    "    except Exception as e:\n",
    "        return {\"status_code\": response.status_code if 'response' in locals() else None,\n",
    "                \"processed_at\": datetime.now(timezone.utc), \"review_comment_title_translated\": None,\n",
    "                \"review_comment_message_tr_translated\": None, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4105a799-b5e7-4ccd-a868-8e279ae47c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame:  1000\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 5\n",
    "source_table = 'default.reviews_to_translate_copy'\n",
    "destination_table = 'default.reviews_to_translate_copy_result'\n",
    "source_df = spark.read.table(source_table)\n",
    "row_count = source_df.count()\n",
    "print(\"Number of rows in the DataFrame: \", row_count)\n",
    "\n",
    "destination_df = spark.read.table(destination_table)\n",
    "destination_delta_table = DeltaTable.forName(spark, destination_table)\n",
    "\n",
    "call_process_data_udf = udf(process_data, StructType([\n",
    "    StructField(\"status_code\", IntegerType(), True),\n",
    "    StructField(\"processed_at\", TimestampType(), True),\n",
    "    StructField(\"review_comment_title_translated\", StringType(), True),\n",
    "    StructField(\"review_comment_message_tr_translated\", StringType(), True),\n",
    "    StructField(\"error\", StringType(), True),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de69d1b4-89aa-4b40-8a9b-30376589f9d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 5\nprocessed: 10\nprocessed: 15\nprocessed: 20\nprocessed: 25\nprocessed: 30\nprocessed: 35\nprocessed: 40\nprocessed: 45\nprocessed: 50\nprocessed: 55\nprocessed: 60\nprocessed: 65\nprocessed: 70\nprocessed: 75\nprocessed: 80\nprocessed: 85\nprocessed: 90\nprocessed: 95\nprocessed: 100\nprocessed: 105\nprocessed: 110\nprocessed: 115\nprocessed: 120\nprocessed: 125\nprocessed: 130\nprocessed: 135\nprocessed: 140\nprocessed: 145\nprocessed: 150\nprocessed: 155\nprocessed: 160\nprocessed: 165\nprocessed: 170\nprocessed: 175\nprocessed: 180\nprocessed: 185\nprocessed: 190\nprocessed: 195\nprocessed: 200\nprocessed: 205\nprocessed: 210\nprocessed: 215\nprocessed: 220\nprocessed: 225\nprocessed: 230\nprocessed: 235\nprocessed: 240\nprocessed: 245\nprocessed: 250\nprocessed: 255\nprocessed: 260\nprocessed: 265\nprocessed: 270\nprocessed: 275\nprocessed: 280\nprocessed: 285\nprocessed: 290\nprocessed: 295\nprocessed: 300\nprocessed: 305\nprocessed: 310\nprocessed: 315\nprocessed: 320\nprocessed: 325\nprocessed: 330\nprocessed: 335\nprocessed: 340\nprocessed: 345\nprocessed: 350\nprocessed: 355\nprocessed: 360\nprocessed: 365\nprocessed: 370\nprocessed: 375\nprocessed: 380\nprocessed: 385\nprocessed: 390\nprocessed: 395\nprocessed: 400\nprocessed: 405\nprocessed: 410\nprocessed: 415\nprocessed: 420\nprocessed: 425\nprocessed: 430\nprocessed: 435\nprocessed: 440\nprocessed: 445\nprocessed: 450\nprocessed: 455\nprocessed: 460\nprocessed: 465\nprocessed: 470\nprocessed: 475\nprocessed: 480\nprocessed: 485\nprocessed: 490\nprocessed: 495\nprocessed: 500\nprocessed: 505\nprocessed: 510\nprocessed: 515\nprocessed: 520\nprocessed: 525\nprocessed: 530\nprocessed: 535\nprocessed: 540\nprocessed: 545\nprocessed: 550\nprocessed: 555\nprocessed: 560\nprocessed: 565\nprocessed: 570\nprocessed: 575\nprocessed: 580\nprocessed: 585\nprocessed: 590\nprocessed: 595\nprocessed: 600\nprocessed: 605\nprocessed: 610\nprocessed: 615\nprocessed: 620\nprocessed: 625\nprocessed: 630\nprocessed: 635\nprocessed: 640\nprocessed: 645\nprocessed: 650\nprocessed: 655\nprocessed: 660\nprocessed: 665\nprocessed: 670\nprocessed: 675\nprocessed: 680\nprocessed: 685\nprocessed: 690\nprocessed: 695\nprocessed: 700\nprocessed: 705\nprocessed: 710\nprocessed: 715\nprocessed: 720\nprocessed: 725\nprocessed: 730\nprocessed: 735\nprocessed: 740\nprocessed: 745\nprocessed: 750\nprocessed: 755\nprocessed: 760\nprocessed: 765\nprocessed: 770\nprocessed: 775\nprocessed: 780\nprocessed: 785\nprocessed: 790\nprocessed: 795\nprocessed: 800\nprocessed: 805\nprocessed: 810\nprocessed: 815\nprocessed: 820\nprocessed: 825\nprocessed: 830\nprocessed: 835\nprocessed: 840\nprocessed: 845\nprocessed: 850\nprocessed: 855\nprocessed: 860\nprocessed: 865\nprocessed: 870\nprocessed: 875\nprocessed: 880\nprocessed: 885\nprocessed: 890\nprocessed: 895\nprocessed: 900\nprocessed: 905\nprocessed: 910\nprocessed: 915\nprocessed: 920\nprocessed: 925\nprocessed: 930\nprocessed: 935\nprocessed: 940\nprocessed: 945\nprocessed: 950\nprocessed: 955\nprocessed: 960\nprocessed: 965\nprocessed: 970\nprocessed: 975\nprocessed: 980\nprocessed: 985\nprocessed: 990\nprocessed: 995\nprocessed: 1000\n"
     ]
    }
   ],
   "source": [
    "is_processed_available = \"processed\" in destination_df.columns\n",
    "if is_processed_available:\n",
    "    destination_df = destination_df.filter((f.col(\"processed\").isNull()) | (f.col(\"processed\") == False))\n",
    "\n",
    "df_count = destination_df.count()\n",
    "\n",
    "num_chunks = (df_count + chunk_size - 1) // chunk_size\n",
    "counter = 0\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    offset = i * chunk_size\n",
    "\n",
    "    if is_processed_available:\n",
    "        chunk_df = spark.sql(\n",
    "            f\"SELECT * FROM {destination_table} where (processed is null or processed = false) order by review_id LIMIT {chunk_size}\")\n",
    "    else:\n",
    "        chunk_df = spark.sql(\n",
    "            f\"SELECT * FROM {destination_table} order by review_id LIMIT {chunk_size} OFFSET {offset}\")\n",
    "\n",
    "    if not chunk_df.rdd.isEmpty():\n",
    "        resultant_df = chunk_df.withColumn(\"Result\",\n",
    "                                           call_process_data_udf(\n",
    "                                               struct([chunk_df[x] for x in chunk_df.columns])))\n",
    "\n",
    "        resultant_df = resultant_df.withColumn(\"status_code\", f.col(\"Result.status_code\"))\n",
    "        resultant_df = resultant_df.withColumn(\"processed_at\", f.col(\"Result.processed_at\"))\n",
    "        resultant_df = resultant_df.withColumn(\"review_comment_title_translated\",\n",
    "                                               f.col(\"Result.review_comment_title_translated\"))\n",
    "        resultant_df = resultant_df.withColumn(\"review_comment_message_tr_translated\",\n",
    "                                               f.col(\"Result.review_comment_message_tr_translated\"))\n",
    "        resultant_df = resultant_df.withColumn(\"error\", f.col(\"Result.error\"))\n",
    "\n",
    "        \"\"\"\n",
    "        review_id has taken as a primary key so we perform merge between destination table\n",
    "        and resultant_df based on this column. \n",
    "        \"\"\"\n",
    "        destination_delta_table.alias('inputs') \\\n",
    "            .merge(\n",
    "            resultant_df.alias('updates'),\n",
    "            f'inputs.review_id = updates.review_id'\n",
    "        ) \\\n",
    "            .whenMatchedUpdate(set=\n",
    "        {\n",
    "            \"status_code\": f.col(\"updates.status_code\"),\n",
    "            \"processed\": lit(True),\n",
    "            \"processed_at\": f.col(\"updates.processed_at\"),\n",
    "            \"review_comment_title_translated\": f.col(\"updates.review_comment_title_translated\"),\n",
    "            \"review_comment_message_tr_translated\": f.col(\"updates.review_comment_message_tr_translated\"),\n",
    "            \"error\": f.col(\"updates.error\")\n",
    "        }\n",
    "        ).execute()\n",
    "\n",
    "        if destination_delta_table.toDF().schema != resultant_df.schema:\n",
    "            destination_delta_table = DeltaTable.forName(spark, destination_table)\n",
    "\n",
    "        counter = counter + chunk_size\n",
    "        print(f\"processed: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace5cf48-d986-4d0a-be4f-017b576818ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "destination_df = spark.read.table(destination_table)\n",
    "destination_df.write.mode(\"overwrite\").format(\"delta\").save(\"/mnt/olist-gold/reviews_eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ec86cd-a554-4e97-881a-447bf5fd3d64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').load(\"/mnt/olist-gold/reviews_eng\")\n",
    "row_count = df.count()\n",
    "print(\"Number of rows in the DataFrame: \", row_count)\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Reviews Text Translation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
