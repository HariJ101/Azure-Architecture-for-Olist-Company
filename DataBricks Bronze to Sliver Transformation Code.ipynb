{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48aefee3-40dd-4b54-831c-9f126125fa10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('mnt/olist-bronze/bronze_layer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f1052f-39f1-44f2-b730-8e3c3aaac68c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_path_trans= '/mnt/olist-bronze/bronze_layer/olist_product_category_name_translation/olist_product_category_name_translation.parquet'\n",
    "input_path_prod = '/mnt/olist-bronze/bronze_layer/olist_products_dataset/olist_products_dataset.parquet'\n",
    "input_path_items = '/mnt/olist-bronze/bronze_layer/olist_order_items_dataset/olist_order_items_dataset.parquet'\n",
    "input_path_orders = '/mnt/olist-bronze/bronze_layer/olist_orders_dataset/olist_orders_dataset.parquet'\n",
    "input_path_payments = '/mnt/olist-bronze/bronze_layer/olist_order_payments_dataset/olist_order_payments_dataset.parquet'\n",
    "input_path_customers = '/mnt/olist-bronze/bronze_layer/olist_customers_dataset/olist_customers_dataset.parquet'\n",
    "input_path_sellers = '/mnt/olist-bronze/bronze_layer/olist_sellers_dataset/olist_sellers_dataset.parquet'\n",
    "input_path_reviews = '/mnt/olist-bronze/bronze_layer/olist_order_reviews_dataset/olist_order_reviews_dataset.parquet'\n",
    "input_path_geo = '/mnt/olist-bronze/bronze_layer/olist_geolocation_dataset/olist_geolocation_dataset.parquet'\n",
    "input_path_census = '/mnt/olist-bronze/bronze_layer/olist_municipalities_pop/olist_municipalities_pop.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587c1427-04d9-44f1-bc0a-88c0acabc57b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e97a3f2-02fe-4f45-b71f-248c8874d0f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "columns = {0: \"product_category_name\", 1: \"product_category_name_english\"}\n",
    "product_trans_df = pd.DataFrame(spark.read.format('parquet').load(input_path_trans,header=True).collect())\n",
    "product_trans_df.rename(columns=columns, inplace=True)\n",
    "print(product_trans_df.head())\n",
    "print(product_trans_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803079dc-f9c3-4a19-b9bf-c067cdc13555",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = {0: \"product_id\", 1: \"product_category_name\", 2:\"product_name_lenght\",3: \"product_description_lenght\", 4: \"product_photos_qty\", 5: \"product_weight_g\",6:\"product_length_cm\",7: \"product_height_cm\",8:\"product_width_cm\"}\n",
    "products_df= pd.DataFrame(spark.read.format('parquet').load(input_path_prod,header=True).collect())\n",
    "products_df.rename(columns=columns, inplace=True)\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "94e519fc-e60e-4426-98a7-5eb2af845668",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = {0: \"order_id\", 1: \"order_item_id\", 2:\"product_id\",3: \"seller_id\", 4: \"shipping_limit_date\", 5: \"price\",6:\"freight_value\"}\n",
    "order_items_df= pd.DataFrame(spark.read.format('parquet').load(input_path_items,header=True).collect())\n",
    "order_items_df.rename(columns=columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77663d15-8a11-4509-b5d9-5a48f0767e9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%store order_items_df\n",
    "print(order_items_df.head())\n",
    "print(order_items_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35588f7-8ee5-41d1-985c-8900e0a5b254",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the data type of the 'product_category_name','product_category_name_english' column to string\n",
    "product_trans_df[['product_category_name','product_category_name_english']] = product_trans_df[['product_category_name','product_category_name_english']].astype('string')\n",
    "print(product_trans_df.isna().sum())\n",
    "print(product_trans_df.nunique)\n",
    "print(product_trans_df.describe())\n",
    "print(product_trans_df.dtypes)\n",
    "\n",
    "%store product_trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "36b9ef14-c982-4aa4-9a06-358e0c703a29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "products_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8b20b8-0c85-4604-9b37-282d5f68343e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%store -r products_df\n",
    "\n",
    "# Rename the column\n",
    "products_df1=products_df.rename(columns={\"product_name_lenght\": \"product_name_length\",\"product_description_lenght\": \"product_description_length\"})\n",
    "\n",
    "# Filter the DataFrame to find rows where product_weight_g is equal to 0 and null\n",
    "weight_rows = products_df1[(products_df1[\"product_weight_g\"] == 0) | (products_df1[\"product_weight_g\"].isnull())]\n",
    "\n",
    "# Replace 0 values in the 'product_weight_g' column with NaN\n",
    "products_df1['product_weight_g'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Assert that there are no 0 values remaining in the 'product_weight_g' column\n",
    "assert not (products_df1['product_weight_g'] == 0).any(), \"Some 0 values are not replaced with NaN.\"\n",
    "\n",
    "# Find rows where product_weight_g is NaN\n",
    "nan_values = products_df1[products_df1[\"product_weight_g\"].isnull()][\"product_weight_g\"]\n",
    "\n",
    "# Change all missing values to NaN\n",
    "products_df1.replace('', np.nan, inplace=True)\n",
    "# Replace null values in multiple columns with \"NULL\" using a dictionary\n",
    "products_df2 = products_df1.fillna({\"product_category_name\": \"NULL\"})\n",
    "\n",
    "# Convert the data type of the 'product_name_length','product_description_length','product_photos_qty' column to integer\n",
    "products_df2[['product_name_length','product_description_length','product_photos_qty']] = products_df2[['product_name_length','product_description_length','product_photos_qty']].astype('Int64')\n",
    "\n",
    "# Convert the data type of the 'product_id','product_category_name' column to string\n",
    "products_df2[['product_id','product_category_name']] = products_df2[['product_id','product_category_name']].astype('string')\n",
    "\n",
    "%store products_df2\n",
    "products_df2.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e098e8c-3f8d-416c-814d-411ea9363e0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%store -r products_df2\n",
    "# Merge the product_trans DataFrame into the products DataFrame based on category names\n",
    "products_df3 = products_df2.merge(product_trans_df, left_on='product_category_name', right_on='product_category_name', how =\"left\")\n",
    "# Display the merged DataFrame\n",
    "products_df3\n",
    "# Replace null values in the product_category_name_english column with \"NULL\" \n",
    "products_df3['product_category_name_english'].fillna(value=\"NULL\", inplace=True)\n",
    "print(products_df3.info())\n",
    "print(products_df3.nunique())\n",
    "%store products_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3780993a-f248-40e8-a8d4-53345d0a79dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%store -r products_df3\n",
    "\n",
    "#replace the \"_\" wtih \" \" to do standardization of text\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].str.replace(\"_\", \" \")\n",
    "\n",
    "# Specific category names have been replaced with more generalized ones for better clarity and consistency in the data\n",
    "\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('fashio female clothing', 'fashion female clothing')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('home confort','home comfort')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('costruction tools tools', 'construction tools')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('construction tools safety', 'construction safety tools')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('telephony','telephones')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('construction tools construction', 'construction tools')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('fashion underwear beach', 'fashion beach underwear')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('kitchen dining laundry garden furniture', 'kitchen, dining, laundry and garden furniture')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('fixed telephony', 'fixed telephones')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('books technical','technical books')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('furniture mattress and upholstery','furniture, mattress and upholstery')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('sports leisure', 'sports and leisure')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('books general interest', 'general interest books')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('fashion bags accessories','fashion bags and accessories')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('watches gifts', 'watches and gifts')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('furniture living room', 'living room furniture')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('books imported', 'imported books')\n",
    "products_df3['product_category_name_english'] = products_df3['product_category_name_english'].replace('health beauty', 'health and beauty')\n",
    "\n",
    "# Extract unique values from the 'product_category_name_english' column\n",
    "unique_categories = products_df3['product_category_name_english'].unique()\n",
    "\n",
    "# Filter the DataFrame based on unique categories\n",
    "products_df4 = products_df3[products_df3['product_category_name_english'].isin(unique_categories)]\n",
    "\n",
    "# Print information about the DataFrame\n",
    "products_df4.info()\n",
    "print(products_df4['product_category_name_english'])\n",
    "\n",
    "# Store the DataFrame for later use\n",
    "%store products_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65ee15a-7b16-4939-b770-03602a1fe865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%store -r order_items_df\n",
    "# Convert the shipping_limit_date column to datetime format\n",
    "order_items_df['shipping_limit_date'] = pd.to_datetime(order_items_df['shipping_limit_date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# no missing values in the order item\n",
    "print(order_items_df.isna().sum())\n",
    "\n",
    "print(order_items_df.nunique())\n",
    "print(order_items_df.dtypes)\n",
    "order_items_df1 = order_items_df.copy()\n",
    "%store order_items_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30022073-70a7-4d85-8278-b1c439b3375c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%store -r order_items_df1\n",
    "order_items_df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318b9f21-0bb7-4376-88d3-6d2916364afa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%store -r order_items_df1\n",
    "# Group by order_id, product_id, seller_id,shipping_limit_date, price and freight_value and count occurrences\n",
    "grouped_df = order_items_df1.groupby(['order_id', 'product_id', 'seller_id', 'shipping_limit_date','price','freight_value']).size().reset_index(name='count')\n",
    "\n",
    "# Calculate the total count for each group\n",
    "order_items_df2 = grouped_df.groupby(['order_id', 'product_id', 'seller_id', 'shipping_limit_date','price','freight_value']).agg({'count': 'sum'}).reset_index()\n",
    "order_items_df2.rename(columns={'count': 'order_quantity', 'price': 'price_per_item', 'freight_value':'freight_value_per_item'}, inplace=True)\n",
    "\n",
    "# Sort the total_counts DataFrame by the 'number_of_items_ordered' column in descending order\n",
    "order_items_df2 = order_items_df2.sort_values(by='order_quantity', ascending=False)\n",
    "\n",
    "# Reset the index of the sorted DataFrame\n",
    "order_items_df2.reset_index(drop=True, inplace=True)\n",
    "%store order_items_df2\n",
    "# Display the sorted DataFrame with reset index\n",
    "order_items_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "69b4a096-ef41-4f00-9a5b-25043af4620c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%store -r order_items_df2\n",
    "print(order_items_df2.nunique())\n",
    "print(order_items_df2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c7cc0c-ce58-4fa4-8ffd-6fa922d36b4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "order_items_spark_df = spark.createDataFrame(products_df4)\n",
    "order_items_spark_df = spark.createDataFrame(order_items_df2)\n",
    "\n",
    "# Write Spark DataFrame to delta\n",
    "order_items_spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/mnt/olist-silver/products\")\n",
    "order_items_spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/mnt/olist-silver/order_items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0c4a45b-9908-4644-bf30-874fef61d095",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transform Orders Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e24df293-07a9-4ade-a8ec-9b878580f9be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "order_payments_df1 = pd.DataFrame(spark.read.format(\"parquet\").load(input_path_payments,header=True).collect())\n",
    "\n",
    "pay_col = {0: \"order_id\", \n",
    "              1: \"payment_sequential\", \n",
    "              2: \"payment_type\", \n",
    "              3: \"payment_installments\", \n",
    "              4: \"payment_value\"}\n",
    "order_payments_df1.rename(columns=pay_col, inplace=True) \n",
    "\n",
    "order_payments_df1 = order_payments_df1.astype({\"order_id\": \"string\",\n",
    "                                   \"payment_sequential\": \"int\", \n",
    "                                   \"payment_type\": \"string\",\n",
    "                                   \"payment_installments\": \"int\", \n",
    "                                   \"payment_value\": \"float\"})\n",
    "order_payments_df1.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc012ccb-5388-4dee-acdb-b4283e06ebcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get information about the dataset\n",
    "order_payments_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a42deb-fe2e-477a-b234-5818612a2e46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop 9 rows where payment_value is 0. Remainder: 103,877 \n",
    "order_payments_df2 = order_payments_df1.copy()\n",
    "order_payments_df2 = order_payments_df2[order_payments_df2['payment_value'] != 0].reset_index(drop=True)\n",
    "\n",
    "# Filter the dataframe for a specific order_id\n",
    "order_id = \"fa65dad1b0e818e3ccc5cb0e39231352\"\n",
    "pay_subset = order_payments_df2[order_payments_df2['order_id'] == order_id]\n",
    "\n",
    "# # Renumber payment_sequential starting from 15\n",
    "pay_subset2 = pay_subset.copy()\n",
    "pay_subset2['payment_sequential'] = pay_subset2['payment_sequential'].apply(lambda x: x - 2 if x > 14 else x)\n",
    "\n",
    "# Drop rows containing the problematic order_id. Remainder: 103,850\n",
    "order_payments_df3 = order_payments_df2.copy()\n",
    "order_payments_df3 = order_payments_df3[order_payments_df3['order_id'] != order_id].reset_index(drop=True)\n",
    "\n",
    "# Then join the remaining dataframe with the renumbered pay_subset2\n",
    "# Remainder: 103,877 \n",
    "order_payments_df4 = pd.concat([order_payments_df3, pay_subset2]) \n",
    "order_payments_df4.reset_index(drop=True)\n",
    "order_payments_df4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c788bda4-e611-465d-8735-a52928463f92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df1 = pd.DataFrame(spark.read.format(\"parquet\").load(input_path_orders,header=True).collect())\n",
    "\n",
    "order_col = {0: \"order_id\", \n",
    "             1: \"customer_id\", \n",
    "             2: \"order_status\", \n",
    "             3: \"order_purchase_timestamp\", \n",
    "             4: \"order_approved_at\", \n",
    "             5: \"order_delivered_carrier_date\", \n",
    "             6: \"order_delivered_customer_date\", \n",
    "             7: \"order_estimated_delivery_date\"}\n",
    "orders_df1.rename(columns=order_col, inplace=True) \n",
    "\n",
    "orders_df1 = orders_df1.astype({\"order_id\": \"string\", \n",
    "                                \"customer_id\": \"string\", \n",
    "                                \"order_status\": \"string\"})\n",
    "\n",
    "# Bhuvana's custom function to parse dates with flexible format handling\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except ValueError:\n",
    "        return pd.NaT\n",
    "\n",
    "# Convert timestamp dtypes \n",
    "date_cols = ['order_purchase_timestamp', 'order_approved_at', \"order_delivered_carrier_date\", \"order_delivered_customer_date\", \"order_estimated_delivery_date\"]\n",
    "orders_df1[date_cols] = orders_df1[date_cols].applymap(parse_date) \n",
    "\n",
    "orders_df1.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4e6f4c6f-1ff2-471f-83d3-d1a57327d0c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fb8e17e8-f626-4068-8e3c-1031db1cb191",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df2 = orders_df1.copy()\n",
    "\n",
    "# Calculate the number of days between different datetime column\n",
    "orders_df2['approved_to_delivered_carrier_days'] = (orders_df2['order_delivered_carrier_date'] - orders_df2['order_approved_at']).dt.days\n",
    "orders_df2['delivered_carrier_to_customer_days'] = (orders_df2['order_delivered_customer_date'] - orders_df2['order_delivered_carrier_date']).dt.days\n",
    "orders_df2['purchase_to_delivered_customer_days'] = (orders_df2['order_delivered_customer_date'] - orders_df2['order_purchase_timestamp']).dt.days\n",
    "orders_df2['estimated_delivery_delay_days'] = (orders_df2['order_delivered_customer_date'] - orders_df2['order_estimated_delivery_date']).dt.days\n",
    "\n",
    "# Calculate mean, median, and mode for each column\n",
    "def calculate_stats(column):\n",
    "    mean = column.mean()\n",
    "    median = column.median()\n",
    "    mode = column.mode()[0]\n",
    "    return mean, median, mode\n",
    "\n",
    "# Initialize a dictionary to store mode values\n",
    "mode_values = {}\n",
    "\n",
    "# Calculate mean, median, and mode for each column\n",
    "columns = ['approved_to_delivered_carrier_days', 'delivered_carrier_to_customer_days', 'purchase_to_delivered_customer_days', 'estimated_delivery_delay_days']\n",
    "for col in columns:\n",
    "    mean, median, mode = calculate_stats(orders_df2[col])\n",
    "    print(f\"\\n'{col}':\")\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Median:\", median)\n",
    "    print(\"Mode:\", mode)\n",
    "    \n",
    "    # Store mode value in the dictionary\n",
    "    mode_values[col] = mode\n",
    "\n",
    "# Display the mode values dictionary\n",
    "# Keep them for later aggregation\n",
    "print(\"\\nMode values dictionary:\")\n",
    "print(mode_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a0b186-9024-436a-85af-c5c93823405c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0465e6b1-bda7-4b37-a0f1-0d4d644d5f08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows with 'delivered' order status\n",
    "delivered_orders = orders_df2[orders_df2['order_status'].isin(['delivered'])]\n",
    "\n",
    "# Look for missing datetime datatype value\n",
    "columns_to_fill = ['order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date']\n",
    "\n",
    "# Fill missing values for 'delivered' orders with modal values: \n",
    "for column in columns_to_fill:\n",
    "    if column == 'order_approved_at':\n",
    "        delivered_orders.loc[delivered_orders[column].isnull(), column] = delivered_orders.loc[delivered_orders[column].isnull(), 'order_purchase_timestamp'] + pd.Timedelta(mode_values['approved_to_delivered_carrier_days'], unit='D')\n",
    "    elif column == 'order_delivered_carrier_date':\n",
    "        delivered_orders.loc[delivered_orders[column].isnull(), column] = delivered_orders.loc[delivered_orders[column].isnull(), 'order_approved_at'] + pd.Timedelta(mode_values['delivered_carrier_to_customer_days'], unit='D')\n",
    "    elif column == 'order_delivered_customer_date':\n",
    "        delivered_orders.loc[delivered_orders[column].isnull(), column] = delivered_orders.loc[delivered_orders[column].isnull(), 'order_delivered_carrier_date'] + pd.Timedelta(mode_values['purchase_to_delivered_customer_days'], unit='D')\n",
    "\n",
    "delivered_orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3534f466-d059-40f3-97b8-187b240660bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Next, drop the \"Status: Delivered\" records from the original dataset: 2963 records remaining \n",
    "orders_df3 = orders_df2.copy()\n",
    "orders_df3 = orders_df2[orders_df2[\"order_status\"] != \"delivered\"].reset_index(drop=True)\n",
    "orders_df3.info()\n",
    "\n",
    "# Then concatenate the delivered_orders subset back into the original dataset (with all missing values filled)\n",
    "orders_df4 = pd.concat([orders_df3, delivered_orders])\n",
    "orders_df4.info()\n",
    "\n",
    "# Finally, drop the four aggregated fields of timestamp differences as they are now outdated (after filling missing values)\n",
    "columns_to_drop = ['approved_to_delivered_carrier_days', \n",
    "                   'delivered_carrier_to_customer_days', \n",
    "                   'purchase_to_delivered_customer_days', \n",
    "                   'estimated_delivery_delay_days']\n",
    "orders_df5 = orders_df4.drop(columns=columns_to_drop)\n",
    "orders_df5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f5d86a-9fda-4443-86fb-31d976886572",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final step: Add five aggregated fields with updated values, as they will be used for further analysis in Transformation #2 \n",
    "orders_df6 = orders_df5.copy() \n",
    "\n",
    "orders_df6['purchased_to_approved_days'] = (orders_df6['order_approved_at'] - orders_df6['order_purchase_timestamp']).dt.days\n",
    "orders_df6['approved_to_delivered_carrier_days'] = (orders_df6['order_delivered_carrier_date'] - orders_df6['order_approved_at']).dt.days\n",
    "orders_df6['delivered_carrier_to_customer_days'] = (orders_df6['order_delivered_customer_date'] - orders_df6['order_delivered_carrier_date']).dt.days\n",
    "orders_df6['purchased_to_delivered_customer_days'] = (orders_df6['order_delivered_customer_date'] - orders_df6['order_purchase_timestamp']).dt.days\n",
    "orders_df6['estimated_delivery_delay_days'] = (orders_df6['order_delivered_customer_date'] - orders_df6['order_estimated_delivery_date']).dt.days\n",
    "\n",
    "orders_df6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f54a3d13-b1e7-49c4-a3e2-0fcc8fdfe6b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df6.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26bb7232-3911-4e1c-b104-1401cf6c23ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "order_payments_spark_df = spark.createDataFrame(order_payments_df4)\n",
    "\n",
    "# Write Spark DataFrame to delta\n",
    "order_payments_spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/mnt/olist-silver/order_payments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708b91ba-210f-41c7-8168-70a4fbb77caa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "orders_spark_df = spark.createDataFrame(orders_df6)\n",
    "\n",
    "# Write Spark DataFrame to delta\n",
    "orders_spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/mnt/olist-silver/orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eab9330f-6cd9-4159-b078-d6a35428be78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transform Geolocation and Sellers Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f60be3e5-ebe6-40c4-aec5-7b9110a1ffe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "census_df1 = pd.DataFrame(spark.read.format('parquet').load(input_path_census,header=True).collect())\n",
    "\n",
    "order_col = {0: \"municipality\", \n",
    "             1: \"population\"\n",
    "            }\n",
    "census_df1.rename(columns=order_col, inplace=True) \n",
    "\n",
    "census_df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "036a9501-96b8-41d8-9410-4939ee537319",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "census_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4307c46f-007e-4cd8-af7b-7ad7e002cba8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split all municipality values into two columns: city and state\n",
    "# Create a new dataframe with lowercased & whitespace-stripped city, state, and population \n",
    "\n",
    "census_df2 = census_df1[\"municipality\"].str.extract(r'(.+)\\((\\w+)\\)').join(census_df1[\"population\"])\n",
    "census_df2.rename(columns={0: \"city\", 1: \"state\"}, inplace=True)\n",
    "census_df2[\"city\"] = census_df2[\"city\"].str.lower().str.strip()\n",
    "\n",
    "# Replaced accented characters \n",
    "accent_dict = {\"á\": \"a\", \n",
    "               \"é\": \"e\", \n",
    "               \"í\": \"i\", \n",
    "               \"ó\": \"o\", \n",
    "               \"ú\": \"u\", \n",
    "               \"à\": \"a\", \n",
    "               \"ò\": \"o\", \n",
    "               \"â\": \"a\", \n",
    "               \"ê\": \"e\", \n",
    "               \"ô\": \"o\", \n",
    "               \"ã\": \"a\", \n",
    "               \"õ\": \"o\", \n",
    "               \"ç\": \"c\", \n",
    "               \"ü\": \"u\"}\n",
    "\n",
    "census_converted = census_df2.copy() \n",
    "census_converted.replace(accent_dict, regex=True, inplace=True)\n",
    "census_converted.tail()\n",
    "\n",
    "census_df3 = census_converted.astype({\"city\": \"string\",\n",
    "                                   \"state\": \"string\", \n",
    "                                   \"population\": \"int\"})\n",
    "census_df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6db1e4cf-462e-40c1-b41d-5d6240bee6fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "census_df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "40bf4694-093a-407c-8bba-67f12668a2db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geolocations_df1 = pd.DataFrame(spark.read.format('parquet').load(input_path_geo,header=True).collect())\n",
    "\n",
    "geolocations_col = {0: \"geolocation_zip_code_prefix\", \n",
    "                    1: \"geolocation_lat\", \n",
    "                    2: \"geolocation_lng\", \n",
    "                    3: \"geolocation_city\", \n",
    "                    4: \"geolocation_state\"}\n",
    "geolocations_df1.rename(columns=geolocations_col, inplace=True) \n",
    "\n",
    "geolocations_df1 = geolocations_df1.astype({\"geolocation_zip_code_prefix\": \"int\",\n",
    "                                   \"geolocation_lat\": \"float\", \n",
    "                                   \"geolocation_lng\": \"float\",\n",
    "                                   \"geolocation_city\": \"string\", \n",
    "                                   \"geolocation_state\": \"string\"})\n",
    "geolocations_df1.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2799b899-f40b-4b93-b0d5-7e2ae6555991",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geolocations_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d7c6b2df-0ce3-4989-956f-50f1e1ddeb77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replaced accented characters \n",
    "geolocations_converted = geolocations_df1.copy() \n",
    "geolocations_converted.replace(accent_dict, regex=True, inplace=True)\n",
    "geolocations_converted.tail()\n",
    "geolocations_converted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7c0c41-79d5-4acf-af72-9492334ba52d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop latitude & longitude outliers (relative to Brazil's range of coordinates)\n",
    "geolocations_df3 = geolocations_converted.copy() \n",
    "\n",
    "geolocations_df3 = geolocations_df3[geolocations_df3[\"geolocation_lat\"] <= 5.288685]\n",
    "assert geolocations_df3[\"geolocation_lat\"].max() <= 5.288685\n",
    "\n",
    "geolocations_df3 = geolocations_df3[geolocations_df3[\"geolocation_lat\"] >= -33.798533]\n",
    "assert geolocations_df3[\"geolocation_lat\"].min() >= -33.798533\n",
    "\n",
    "geolocations_df3 = geolocations_df3[geolocations_df3[\"geolocation_lng\"] <= -34.703311]\n",
    "assert geolocations_df3[\"geolocation_lng\"].max() <= -34.703311\n",
    "\n",
    "geolocations_df3 = geolocations_df3[geolocations_df3[\"geolocation_lng\"] >= -73.968899]\n",
    "assert geolocations_df3[\"geolocation_lng\"].min() >= -73.968899 \n",
    "\n",
    "# Group records by zip_code_prefix, getting mean coordinates & modal city and state names \n",
    "# Then reset index to make zip_code_prefix a column \n",
    "geolocations_df4 = geolocations_df3.copy()\n",
    "summaries = {\"geolocation_lat\": \"mean\", \n",
    "             \"geolocation_lng\": \"mean\", \n",
    "             \"geolocation_city\": pd.Series.mode, \n",
    "             \"geolocation_state\": pd.Series.mode}\n",
    "geolocations_df4 = geolocations_df4.groupby(by=\"geolocation_zip_code_prefix\").agg(summaries)\n",
    "geolocations_df4.head()\n",
    "assert len(geolocations_df4) == len(geolocations_df3[\"geolocation_zip_code_prefix\"].unique())\n",
    "geolocations_df5 = geolocations_df4.reset_index() \n",
    "geolocations_df5['geolocation_city'] = geolocations_df5['geolocation_city'].astype('string')\n",
    "\n",
    "geolocations_df5.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4918b4d5-e098-49ef-b49f-419b2eb71a04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df1 = pd.DataFrame(spark.read.format('parquet').load(input_path_customers,header=True).collect())\n",
    "\n",
    "customers_col = {0: \"customer_id\", \n",
    "                    1: \"customer_unique_id\", \n",
    "                    2: \"customer_zip_code_prefix\", \n",
    "                    3: \"customer_city\", \n",
    "                    4: \"customer_state\"}\n",
    "customers_df1.rename(columns=customers_col, inplace=True) \n",
    "\n",
    "customers_df1 = customers_df1.astype({\"customer_id\": \"string\",\n",
    "                                   \"customer_unique_id\": \"string\", \n",
    "                                   \"customer_zip_code_prefix\": \"int\",\n",
    "                                   \"customer_city\": \"string\", \n",
    "                                   \"customer_state\": \"string\"})\n",
    "customers_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0e9e2fa5-c911-45ac-9a2b-1835d617f249",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ca312d8d-dbba-4daf-ada3-78df3af3e714",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SOMEWHAT USEFUL: Standardisation #1 Use Geolocations to change names\n",
    "customers_df2 = customers_df1.copy() \n",
    "customers_df2[\"city\"] = customers_df2[\"customer_city\"] \n",
    "\n",
    "# Subset of Customers with unstandardised city names: 253 unique city names \n",
    "unstan_cities = set(customers_df2[\"city\"]).difference(set(census_df3[\"city\"]))\n",
    "unstan_df = customers_df2[customers_df2[\"city\"].isin(unstan_cities)]\n",
    "\n",
    "# Use the collapsed copy of Geolocations with one record per municipality zip code: geolocations_df5\n",
    "# 5039 Geolocations cities that are official municpalities   \n",
    "official_cities = set(geolocations_df5[\"geolocation_city\"]).intersection(set(census_df3[\"city\"]))\n",
    "\n",
    "# Subset of Geolocations that contains official municipality names \n",
    "official_geo = geolocations_df5[geolocations_df5[\"geolocation_city\"].isin(official_cities)]\n",
    "official_geo2 = official_geo[[\"geolocation_zip_code_prefix\", \"geolocation_city\"]] \n",
    "%store official_geo2\n",
    "\n",
    "# Match the subset to the 293 unstandardised city names of the Customers subset \n",
    "customers_subset = unstan_df.merge(official_geo2, \n",
    "                            left_on=\"customer_zip_code_prefix\",\n",
    "                            right_on=\"geolocation_zip_code_prefix\", \n",
    "                            how=\"left\",\n",
    "                            suffixes=(\"_\", \"\"))\n",
    "customers_subset.head()\n",
    "\n",
    "# Created a column \"official_city\" with all 30 official municipality names having replaced initial names \n",
    "customers_subset[\"geolocation_city\"] = customers_subset[\"geolocation_city\"].fillna(customers_subset[\"city\"])\n",
    "customers_subset2 = customers_subset[[\"customer_id\", \n",
    "                        \"customer_unique_id\", \n",
    "                        \"customer_zip_code_prefix\", \n",
    "                        \"customer_city\", \n",
    "                        \"customer_state\", \n",
    "                        \"geolocation_city\"]]\n",
    "customers_subset2.rename(columns={\"geolocation_city\": \"official_city\"}, inplace=True)\n",
    "customers_subset2.head()\n",
    "\n",
    "# Replacing unstandardised city names in Customers dataset with official names, matched by zip code \n",
    "customers_subset3 = customers_subset2.copy() \n",
    "customers_df3 = customers_df2.copy()\n",
    "customers_df3[\"city\"].update(customers_df3[\"customer_id\"].map(customers_subset3.set_index(\"customer_id\")[\"official_city\"]))\n",
    "customers_df3.rename(columns={\"city\": \"official_city\"}, inplace=True)\n",
    "\n",
    "# Summary: Out of the 253 unstandardised Customers' unique city names, \n",
    "# 26 have been standardised here - leaving 227 unstandardised. \n",
    "len(set(customers_df3[\"official_city\"]).difference(set(census_df3[\"city\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0025a26-9ff8-4348-b249-437dc9b505ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardisation #2 Replacing 230 unstandardised city names with official demographic city names, \n",
    "# then testing with assert statements \n",
    "\n",
    "customers_df4 = customers_df3.copy() \n",
    "city_dict = {\"abrantes\": \"camacari\",\n",
    "             \"adhemar de barros\": \"terra rica\", \n",
    "             'agisse': \"rancharia\",\n",
    "             'aguas claras': \"viamao\",\n",
    "             'alexandra': \"paranagua\",\n",
    "             'alexandrita': \"iturama\",\n",
    "             'alto alegre do iguacu': \"capitao leonidas marques\",\n",
    "             'alto sao joao': \"roncador\",\n",
    "             'amanari': \"maranguape\",\n",
    "             'amparo da serra': \"amparo do serra\",\n",
    "             'andrequice': \"tres marias\",\n",
    "             'angelo frechiani': \"colatina\",\n",
    "             'angustura': \"alem paraiba\",\n",
    "             'anhandui': \"campo grande\",\n",
    "             'anta': \"sapucaia\",\n",
    "             'antonio pereira': \"ouro preto\",\n",
    "             'antunes': \"igaratinga\",\n",
    "             'aparecida de monte alto': \"monte alto\",\n",
    "             'aparecida de sao manuel': \"sao manuel\", \n",
    "             'araguaia': \"marechal floriano\",\n",
    "             'areia branca dos assis': \"mandirituba\",\n",
    "             'arembepe': \"camacari\",\n",
    "             'aribice': \"euclides da cunha\",\n",
    "             'arraial d ajuda': \"porto seguro\",\n",
    "             \"arraial d'ajuda\": \"porto seguro\",\n",
    "             'arrozal': \"pirai\",\n",
    "             'avelar': \"paty do alferes\",\n",
    "             'azurita': \"mateus leme\",\n",
    "             'bacaxa': \"saquarema\",\n",
    "             'baguari': \"governador valadares\",\n",
    "             \"bandeirantes d'oeste\": \"sud mennucci\",\n",
    "             'barao ataliba nogueira': \"itapira\",\n",
    "             'barao de juparana': \"valenca\",\n",
    "             'barra de sao joao': \"casimiro de abreu\",\n",
    "             'barra do tarrachil': \"chorrocho\",\n",
    "             'bemposta': \"tres rios\",\n",
    "             'biritiba-mirim': \"biritiba mirim\",\n",
    "             'bom jesus do querendo': \"natividade\",\n",
    "             'bonfim paulista': \"ribeirao preto\",\n",
    "             'botelho': \"santa adelia\",\n",
    "             'braco do rio': \"conceicao da barra\",\n",
    "             'brasopolis': \"brazopolis\", \n",
    "             'cachoeira do brumado': \"mariana\",\n",
    "             'cachoeira do campo': \"ouro preto\",\n",
    "             'california da barra': \"barra do pirai\",\n",
    "             'cambiasca': \"sao fidelis\",\n",
    "             'campo alegre de minas': \"resplendor\",\n",
    "             'capao da porteira': \"viamao\",\n",
    "             'caraiba': \"ibimirim\",\n",
    "             'carnaiba do sertao': \"juazeiro\",\n",
    "             'catu de abrantes': \"lauro de freitas\",\n",
    "             'celina': \"alegre\",\n",
    "             'central de santa helena': \"divino das laranjeiras\",\n",
    "             'chaveslandia': \"santa vitoria\",\n",
    "             'cipo-guacu': \"embu-guacu\",\n",
    "             'cocais': \"barao de cocais\",\n",
    "             'colonia castrolanda': \"castro\",\n",
    "             'conceicao da ibitipoca': \"lima duarte\",\n",
    "             'conceicao do formoso': \"santos dumont\",\n",
    "             'conrado': \"miguel pereira\",\n",
    "             'couto de magalhaes': \"couto de magalhaes de minas\",\n",
    "             'cuite velho': \"conselheiro pena\",\n",
    "             'desembargador otoni': \"diamantina\",\n",
    "             'doce grande': \"quitandinha\",\n",
    "             'domiciano ribeiro': \"ipameri\",\n",
    "             'engenheiro balduino': \"monte aprazivel\",\n",
    "             'engenheiro passos': \"resende\",\n",
    "             'espigao': \"regente feijo\",\n",
    "             'espigao do oeste': \"espigao d'oeste\",\n",
    "             'estevao de araujo': \"araponga\",\n",
    "             'florinia': \"florinea\",\n",
    "             'fonseca': \"alvinopolis\", \n",
    "             'glaura': \"ouro preto\",\n",
    "             'goitacazes': \"campos dos goytacazes\",\n",
    "             'governador portela': \"miguel pereira\",\n",
    "             'graccho cardoso': \"gracho cardoso\",\n",
    "             'grao para': \"grao-para\",\n",
    "             'guarapua': \"dois corregos\",\n",
    "             'guassusse': \"oros\",\n",
    "             'guinda': \"diamantina\",\n",
    "             'hidreletrica tucurui': \"tucurui\",\n",
    "             'holambra ii': \"paranapanema\",\n",
    "             'humildes': \"feira de santana\",\n",
    "             'ibiajara': \"rio do pires\",\n",
    "             'ibiraja': \"itanhem\",\n",
    "             'ibitioca': \"campos dos goytacazes\",\n",
    "             'ibitira': \"rio do antonio\",\n",
    "             'ibitiuva': \"pitangueiras\",\n",
    "             'ilha dos valadares': \"paranagua\",\n",
    "             'ipiabas': \"barra do pirai\",\n",
    "             'irape': \"chavantes\",\n",
    "             'itabatan': \"mucuri\",\n",
    "             'itacurussa': \"mangaratiba\",\n",
    "             'itaipava': \"itapemirim\",\n",
    "             'itapage': \"itapaje\",\n",
    "             'jacare': \"cabreuva\",\n",
    "             'jacigua': \"vargem alta\",\n",
    "             'jaguarembe': \"itaocara\",\n",
    "             'jamaica': \"dracena\",\n",
    "             'jamapara': \"sapucaia\",\n",
    "             'japuiba': \"cachoeiras de macacu\",\n",
    "             'jardim abc de goias': \"cidade ocidental\",\n",
    "             'jaua': \"camacari\",\n",
    "             'lidice': \"rio claro\",\n",
    "             'luizlandia do oeste': \"sao goncalo do abaete\",\n",
    "             'luziapolis': \"campo alegre\",\n",
    "             'macuco de minas': \"itumirim\",\n",
    "             'maioba': \"paco do lumiar\",\n",
    "             'major porto': \"patos de minas\",\n",
    "             'mariental': \"lapa\",\n",
    "             'maristela': \"laranjal paulista\",\n",
    "             'martinesia': \"uberlandia\",\n",
    "             'missi': \"iraucuba\",\n",
    "             'mogi-guacu': \"mogi guacu\",\n",
    "             'monnerat': \"duas barras\",\n",
    "             'monte alverne': \"santa cruz do sul\",\n",
    "             'monte bonito': \"pelotas\",\n",
    "             'monte gordo': \"camacari\",\n",
    "             'monte verde': \"camanducaia\",\n",
    "             'morro de sao paulo': \"cairu\",\n",
    "             'morro do ferro': \"oliveira\",\n",
    "             'morro vermelho': \"caete\",\n",
    "             'murucupi': \"barcarena\",\n",
    "             'mutum parana': \"porto velho\",\n",
    "             'nossa senhora de caravaggio': \"nova veneza\",\n",
    "             'nossa senhora do o': \"ipojuca\",\n",
    "             'nossa senhora do remedio': \"salesopolis\",\n",
    "             \"olhos d'agua\": \"belo horizonte\",\n",
    "             'osvaldo kroeff': \"cambara do sul\", \n",
    "             'pacotuba': \"cachoeiro de itapemirim\",\n",
    "             'padre gonzales': \"tres passos\",\n",
    "             'palmeirinha': \"guarapuava\",\n",
    "             'palmital de minas': \"cabeceira grande\",\n",
    "             'papucaia': \"cachoeiras de macacu\",\n",
    "             'paraju': \"domingos martins\",\n",
    "             'parati': \"paraty\",\n",
    "             'passa tres': \"rio claro\",\n",
    "             'pedra menina': \"rio vermelho\",\n",
    "             'perola independente': \"maripa\",\n",
    "             'perpetuo socorro': \"belo oriente\",\n",
    "             'piao': \"sao jose do vale do rio preto\",\n",
    "             \"picarras\": \"balneario picarras\", \n",
    "             'pindare mirim': \"igarape do meio\",\n",
    "             'pinhotiba': \"eugenopolis\",\n",
    "             'pitanga de estrada': \"mamanguape\",\n",
    "             'piumhii': \"piumhi\",\n",
    "             'poco de pedra': \"sao goncalo do amarante\",\n",
    "             'pocoes de paineiras': \"paineiras\",\n",
    "             'polo petroquimico de triunfo': \"triunfo\",\n",
    "             'ponto do marambaia': \"carai\",\n",
    "             'portela': \"itaocara\",\n",
    "             'porto trombetas': \"oriximina\",\n",
    "             \"posto da mata\": \"nova vicosa\", \n",
    "             'prudencio thomaz': \"rio brilhante\",\n",
    "             'purilandia': \"porciuncula\",\n",
    "             'quatro bocas': \"tome-acu\",\n",
    "             'queixada': \"novo cruzeiro\",\n",
    "             'quilometro 14 do mutum': \"baixo guandu\",\n",
    "             'rainha do mar': \"xangri-la\",\n",
    "             'raposo': \"itaperuna\",\n",
    "             'ravena': \"sabara\",\n",
    "             'rechan': \"itapetininga\",\n",
    "             'ribeiro junqueira': \"leopoldina\",\n",
    "             \"sacra familia do tingua\": \"engenheiro paulo de frontin\",\n",
    "             \"salobro\": \"canarana\",\n",
    "             'santa barbara d oeste': \"santa barbara d'oeste\", \n",
    "             'sanga puita': \"ponta pora\",\n",
    "             'santa cruz do prata': \"guaranesia\",\n",
    "             'santa cruz do timbo': \"porto uniao\",\n",
    "             'santa isabel do para': \"santa izabel do para\",\n",
    "             'santa isabel do rio preto': \"valenca\",\n",
    "             'santa rita da floresta': \"cantagalo\",\n",
    "             'santa rita do ibitipoca': \"santa rita de ibitipoca\",\n",
    "             'santana do capivari': \"pouso alto\",\n",
    "             'santana do livramento': \"sant'ana do livramento\",\n",
    "             'santana do sobrado': \"casa nova\",\n",
    "             'santanesia': \"pirai\",\n",
    "             'santo antonio das queimadas': \"jurema\",\n",
    "             'santo antonio do canaa': \"santa teresa\",\n",
    "             'santo antonio do leverger': \"santo antonio de leverger\", \n",
    "             \"santo antonio dos campos\": \"divinopolis\",\n",
    "             \"santo eduardo\": \"campos dos goytacazes\", \n",
    "             \"sao clemente\": \"santa helena\",\n",
    "             \"sao francisco do humaita\": \"mutum\",\n",
    "             \"sao geraldo do baguari\": \"sao joao evangelista\",\n",
    "             \"sao goncalo do rio das pedras\": \"serro\", \n",
    "             \"sao joao de petropolis\": \"santa teresa\",\n",
    "             \"sao joao do sobrado\": \"pinheiros\",\n",
    "             \"sao jorge do oeste\": \"sao jorge d'oeste\",\n",
    "             \"sao jose do ribeirao\": \"bom jardim\",\n",
    "             \"sao jose do turvo\": \"barra do pirai\",\n",
    "             \"sao mateus de minas\": \"camanducaia\", \n",
    "             \"sao miguel do cambui\": \"marialva\", \n",
    "             \"sao sebastiao da serra\": \"brotas\",\n",
    "             \"sao sebastiao de campos\": \"campos dos goytacazes\",\n",
    "             \"sao sebastiao do paraiba\": \"cantagalo\", \n",
    "             \"sao thome das letras\": \"tres coracoes\",\n",
    "             \"sao vitor\": \"governador valadares\",\n",
    "             \"sede alvorada\": \"cascavel\",\n",
    "             \"serra bonita\": \"buritis\",\n",
    "             \"serra dos dourados\": \"umuarama\",\n",
    "             \"silvano\": \"patrocinio\",\n",
    "             \"silveira carvalho\": \"barao de monte alto\",\n",
    "             \"siriji\": \"sao vicente ferrer\", \n",
    "             \"sucesso\": \"tamboril\", \n",
    "             \"taperuaba\": \"sobral\",\n",
    "             \"tapinas\": \"itapolis\", \n",
    "             \"tecainda\": \"martinopolis\",\n",
    "             \"termas de ibira\": \"ibira\",\n",
    "             \"tocos\": \"campos dos goytacazes\",\n",
    "             \"trancoso\": \"porto seguro\",\n",
    "             \"tres irmaos\": \"cambuci\",\n",
    "             \"tuparece\": \"medina\", \n",
    "             \"valao do barro\": \"sao sebastiao do alto\",\n",
    "             \"vargem grande do soturno\": \"cachoeiro de itapemirim\",\n",
    "             \"venda branca\": \"casa branca\",\n",
    "             \"vermelho\": \"muriae\",\n",
    "             \"vila dos cabanos\": \"barcarena\",\n",
    "             \"vila muriqui\": \"mangaratiba\",\n",
    "             \"vila nelita\": \"agua doce do norte\",\n",
    "             \"vila nova\": \"toledo\",\n",
    "             \"vila pereira\": \"nanuque\",\n",
    "             \"vila reis\": \"apucarana\",\n",
    "             \"visconde de maua\": \"resende\", \n",
    "             \"vitorinos\": \"alto rio doce\",\n",
    "             \"werneck\": \"paraiba do sul\", \n",
    "             \"arace\": \"domingos martins\",\n",
    "             \"brejo bonito\": \"cruzeiro da fortaleza\", \n",
    "             'carajas': \"canaa dos carajas\",\n",
    "             'conservatoria': \"valenca\",\n",
    "             'itamira': \"apora\",\n",
    "             'quatituba': \"itueta\",\n",
    "             'santo amaro de campos': \"campos dos goytacazes\",\n",
    "             'travessao': \"campos dos goytacazes\"} \n",
    "customers_df4.replace({\"official_city\": city_dict}, inplace=True) \n",
    "\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"abrantes\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"adhemar de barros\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"agisse\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"aguas claras\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"alexandra\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"alexandrita\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"alto alegre do iguacu\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"alto sao joao\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"amanari\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"amparo da serra\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"andrequice\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"angelo frechiani\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"angustura\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"anhandui\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"anta\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"antonio pereira\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"antunes\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"aparecida de monte alto\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"aparecida de sao manuel\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"araguaia\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"areia branca dos assis\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"arembepe\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"aribice\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"arraial d ajuda\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"arraial d'ajuda\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"arrozal\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"avelar\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"azurita\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"bacaxa\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"baguari\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"bandeirantes d'oeste\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"barao ataliba nogueira\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"barao de juparana\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"barra de sao joao\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"barra do tarrachil\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"bemposta\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"biritiba-mirim\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"bom jesus do querendo\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"bonfim paulista\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"botelho\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"braco do rio\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"brasopolis\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"cachoeira do brumado\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"cachoeira do campo\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"california da barra\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"cambiasca\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"campo alegre de minas\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"capao da porteira\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"caraiba\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"carnaiba do sertao\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"catu de abrantes\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"celina\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"central de santa helena\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"chaveslandia\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"cipo-guacu\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"cocais\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"colonia castrolanda\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"conceicao da ibitipoca\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"conceicao do formoso\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"conrado\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"couto de magalhaes\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"cuite velho\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"desembargador otoni\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"doce grande\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"domiciano ribeiro\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"engenheiro balduino\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"engenheiro passos\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"espigao\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"espigao do oeste\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"estevao de araujo\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"florinia\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"fonseca\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"glaura\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"goitacazes\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"governador portela\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"graccho cardoso\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"grao para\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"guarapua\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"guassusse\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"guinda\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"hidreletrica tucurui\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"holambra ii\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"humildes\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"ibiajara\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"ibiraja\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"ibitioca\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"ibitira\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"ibitiuva\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"ilha dos valadares\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"ipiabas\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"irape\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"itabatan\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"itacurussa\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"itaipava\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"itapage\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"jacare\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"jacigua\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"jaguarembe\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"jamaica\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"jamapara\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"japuiba\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"jardim abc de goias\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"jaua\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"lidice\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"luizlandia do oeste\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"luziapolis\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"macuco de minas\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"maioba\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"major porto\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"mariental\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"maristela\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"martinesia\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"missi\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"mogi-guacu\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"monnerat\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"monte alverne\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"monte bonito\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"monte gordo\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"morro de sao paulo\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"morro do ferro\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"morro vermelho\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"murucupi\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"mutum parana\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"nossa senhora de caravaggio\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"nossa senhora do o\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"nossa senhora do remedio\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"olhos d'agua\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"osvaldo kroeff\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"pacotuba\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"padre gonzales\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"palmeirinha\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"palmital de minas\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"papucaia\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"paraju\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"parati\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"passa tres\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"pedra menina\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"perola independente\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"perpetuo socorro\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"piao\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"picarras'\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"pindare mirim\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"pinhotiba\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"pitanga de estrada\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"piumhii\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"poco de pedra\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"pocoes de paineiras\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"polo petroquimico de triunfo\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"ponto do marambaia\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"portela\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"porto trombetas\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"posto da mata\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"prudencio thomaz\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"purilandia\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"quatro bocas\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"queixada\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"quilometro 14 do mutum\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"rainha do mar\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"raposo\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"ravena\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"rechan\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"ribeiro junqueira\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sacra familia do tingua\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"salobro\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sanga puita\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santa barbara d oeste\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santa cruz do prata\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santa cruz do timbo\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santa isabel do para\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santa isabel do rio preto\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santa rita da floresta\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santa rita do ibitipoca\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santana do capivari\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santana do livramento\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santana do sobrado\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santanesia\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santo antonio das queimadas\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santo antonio do canaa\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santo antonio do leverger\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santo antonio dos campos\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santo eduardo\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao clemente\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao francisco do humaita\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao geraldo do baguari\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao goncalo do rio das pedras\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao joao de petropolis\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao joao do sobrado\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao jorge do oeste\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao jose do ribeirao\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao jose do turvo\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao mateus de minas\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao miguel do cambui\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao sebastiao da serra\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao sebastiao de campos\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao sebastiao do paraiba\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao thome das letras\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sao vitor\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sede alvorada\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"serra bonita\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"serra dos dourados\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"silvano\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"silveira carvalho\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"siriji\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"sucesso\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"taperuaba\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"tapinas\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"tecainda\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"termas de ibira\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"tocos\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"trancoso\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"tres irmaos\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"tuparece\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"valao do barro\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"vargem grande do soturno\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"venda branca\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"vermelho\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"vila dos cabanos\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"vila muriqui\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"vila nelita\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"vila nova\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"vila pereira\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"vila reis\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"visconde de maua\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"vitorinos\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"werneck\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"arace\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"brejo bonito\"].empty == True\n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"carajas\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"conservatoria\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"itamira\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"quatituba\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"santo amaro de campos\"].empty == True \n",
    "assert customers_df4[customers_df4[\"official_city\"] == \"travessao\"].empty == True \n",
    "\n",
    "customers_df4.info()\n",
    "\n",
    "# Summary: All 230 unique city names have been unstandardised \n",
    "len(set(customers_df4[\"official_city\"]).difference(set(census_df3[\"city\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "050120b4-f556-470b-be64-ec361b7274fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sellers_df1 = pd.DataFrame(spark.read.format('parquet').load(input_path_sellers,header=True).collect())\n",
    "\n",
    "sellers_col = {0: \"seller_id\", \n",
    "                    1: \"seller_zip_code_prefix\", \n",
    "                    2: \"seller_city\", \n",
    "                    3: \"seller_state\"}\n",
    "sellers_df1.rename(columns=sellers_col, inplace=True) \n",
    "\n",
    "sellers_df1 = sellers_df1.astype({\"seller_id\": \"string\",\n",
    "                                   \"seller_zip_code_prefix\": \"int\",\n",
    "                                   \"seller_city\": \"string\", \n",
    "                                   \"seller_state\": \"string\"})\n",
    "sellers_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "94985c60-24eb-45ca-a9e7-c05a0437ec6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sellers_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42551de2-f4fc-4d72-b3b1-c0d2142ae3ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# HIGHLY EFFECTIVE: Standardisation #1 Using Geolocations to change names \n",
    "sellers_df2 = sellers_df1.copy()\n",
    "sellers_df2[\"city\"] = sellers_df2[\"seller_city\"]\n",
    "\n",
    "# Subset of Customers with unstandardised city names: 71 unique city names \n",
    "unstan_cities2 = set(sellers_df2[\"city\"]).difference(set(census_df3[\"city\"]))\n",
    "unstan_df2 = sellers_df2[sellers_df2[\"city\"].isin(unstan_cities2)]\n",
    "\n",
    "# Use the subset of Geolocations that contains official municipality names: official_geo2\n",
    "# Match the subset to the 71 unstardardised city names of the Sellers subset \n",
    "sellers_subset = unstan_df2.merge(official_geo2, \n",
    "                            left_on=\"seller_zip_code_prefix\",\n",
    "                            right_on=\"geolocation_zip_code_prefix\", \n",
    "                            how=\"left\",\n",
    "                            suffixes=(\"_\", \"\"))\n",
    "sellers_subset.head()\n",
    "\n",
    "# Created a column \"official_city\" with all 30 official municipality names having replaced initial names \n",
    "sellers_subset[\"geolocation_city\"] = sellers_subset[\"geolocation_city\"].fillna(sellers_subset[\"city\"])\n",
    "sellers_subset2 = sellers_subset[[\"seller_id\", \n",
    "                        \"seller_zip_code_prefix\", \n",
    "                        \"seller_city\", \n",
    "                        \"seller_state\", \n",
    "                        \"geolocation_city\"]]\n",
    "sellers_subset2.rename(columns={\"geolocation_city\": \"official_city\"}, inplace=True)\n",
    "sellers_subset2.head()\n",
    "\n",
    "# Replacing unstandardised city names in Sellers dataset with official names, matched by zip code \n",
    "sellers_subset3 = sellers_subset2.copy()\n",
    "sellers_df3 = sellers_df2.copy()\n",
    "sellers_df3[\"city\"].update(sellers_df3[\"seller_id\"].map(sellers_subset3.set_index(\"seller_id\")[\"official_city\"]))\n",
    "sellers_df3.rename(columns={\"city\": \"official_city\"}, inplace=True)  \n",
    "\n",
    "# Summary: Out of the 71 unstandardised Sellers' unique city names, 70 have been standardised here. \n",
    "len(set(sellers_df3[\"official_city\"]).difference(set(census_df3[\"city\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f31e2286-5d8d-491f-90d4-5fd65cbfce54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardising #2 Replacing unstandardised city name with official demographic city name, \n",
    "# then testing with assert statement \n",
    "sellers_df4 = sellers_df3.copy()\n",
    "sellers_df4.replace({\"arraial d'ajuda (porto seguro)\": \"porto seguro\"}, inplace=True) \n",
    "\n",
    "assert sellers_df4[sellers_df4[\"official_city\"] == \"arraial d'ajuda (porto seguro)\"].empty == True\n",
    "\n",
    "sellers_df4.info()\n",
    "\n",
    "# Summary: The 1 remaining unstandardised Sellers' unique city name has been standardised. \n",
    "len(set(sellers_df4[\"official_city\"]).difference(set(census_df3[\"city\"]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42bccca-f8c9-4e8f-8775-da6e723338c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imputation of missing zip_code_prefixes from Sellers & Customers into Geolocations \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# First, create a list of Sellers' zipcodes to be added (missing from Geolocations) \n",
    "zip_sell_list = list(set(sellers_df4[\"seller_zip_code_prefix\"]).difference(set(geolocations_df5[\"geolocation_zip_code_prefix\"])))\n",
    "zip_sell_list.sort()\n",
    "\n",
    "# Second, identify each missing zipcode's closest zipcode \n",
    "\n",
    "def closest(lst, missing_zip): \n",
    "    lst = np.asarray(lst)\n",
    "    idx = (np.abs(lst - missing_zip)).argmin()\n",
    "    return lst[idx]\n",
    "\n",
    "lst = geolocations_df5[\"geolocation_zip_code_prefix\"].tolist()\n",
    "subs_zip = []\n",
    "for missing_zip in zip_sell_list: \n",
    "    subs_zip.append(closest(lst, missing_zip))\n",
    "    \n",
    "# Third, create a dictionary of matching \"closest zipcodes\" and \"missing zipcodes\" \n",
    "sell_dict = dict(zip(subs_zip, zip_sell_list))\n",
    "\n",
    "# Fourth, identify the records containing the \"closest zipcodes\" \n",
    "sell_zip_df = geolocations_df5[geolocations_df5[\"geolocation_zip_code_prefix\"].isin(subs_zip)]\n",
    "\n",
    "# Fifth, create a new dataframe by replacing the \"closest zipcodes\" with their \n",
    "# matching \"missing zipcodes\" \n",
    "sell_zip_df2 = sell_zip_df.copy()\n",
    "sell_zip_df2[\"geolocation_zip_code_prefix\"] = sell_zip_df2[\"geolocation_zip_code_prefix\"].map(sell_dict)\n",
    "\n",
    "# Sixth, concatenate the original Geolocations dataframe with the new dataframe \n",
    "geolocations_df6 = pd.concat([geolocations_df5, sell_zip_df2])\n",
    "geolocations_df6.reset_index(drop=True)\n",
    "# %store geolocations_df6\n",
    "\n",
    "# Check if there are still Sellers' zipcodes missing from Geolocations\n",
    "missing_zipcodes0 = list(set(sellers_df4[\"seller_zip_code_prefix\"]).difference(set(geolocations_df6[\"geolocation_zip_code_prefix\"])))\n",
    "if not missing_zipcodes0:\n",
    "    print(\"All sellers' zipcodes are now present in the Geolocations dataframe.\")\n",
    "else:\n",
    "    print(f\"The following {len(missing_zipcodes0)} zipcodes are still missing from the Geolocations dataframe: {missing_zipcodes0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf903bd-a119-4cf0-9d7e-63e832b7f7cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The no. of unique geolocations_zip_code_prefix has increased by 7 (from Sellers) to 19,017\n",
    "geolocations_df6.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8a03126-cb70-437f-90e0-7c0d1fb3ed67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of customers' zipcodes that are missing from the Geolocations dataframe\n",
    "zip_cust_list = list(set(customers_df4[\"customer_zip_code_prefix\"]).difference(set(geolocations_df6[\"geolocation_zip_code_prefix\"])))\n",
    "zip_cust_list.sort()\n",
    "print(f\"Number of missing zipcodes: {len(zip_cust_list)}\")\n",
    "\n",
    "lst2 = geolocations_df6[\"geolocation_zip_code_prefix\"].tolist()\n",
    "\n",
    "if not zip_cust_list:\n",
    "    print(\"All customers' zipcodes are now present in the Geolocations dataframe.\")\n",
    "else:\n",
    "    print(f\"The following {len(zip_cust_list)} zipcodes are still missing from the Geolocations dataframe: {zip_cust_list}\")\n",
    "    \n",
    "    # Iteratively repeat the procedure until no further missing zipcodes are found\n",
    "    while zip_cust_list:\n",
    "        geolocations_df5 = geolocations_df6\n",
    "        \n",
    "        subs_zip3 = []\n",
    "        for missing_zip3 in zip_cust_list:\n",
    "            subs_zip3.append(closest(lst2, missing_zip3))\n",
    "        \n",
    "        cust_dict2 = dict(zip(subs_zip3, zip_cust_list))\n",
    "        \n",
    "        cust_zip_df2 = geolocations_df5[geolocations_df5[\"geolocation_zip_code_prefix\"].isin(subs_zip3)]\n",
    "        cust_zip_df3 = cust_zip_df2.copy()\n",
    "        cust_zip_df3[\"geolocation_zip_code_prefix\"] = cust_zip_df3[\"geolocation_zip_code_prefix\"].map(cust_dict2)\n",
    "        \n",
    "        geolocations_df6 = pd.concat([geolocations_df5, cust_zip_df3])\n",
    "        geolocations_df6.reset_index(drop=True)\n",
    "        print(\"Looping...\")\n",
    "        \n",
    "        zip_cust_list = list(set(customers_df4[\"customer_zip_code_prefix\"]).difference(set(geolocations_df6[\"geolocation_zip_code_prefix\"])))\n",
    "        \n",
    "    print(\"All customers' zipcodes are now present in the Geolocations dataframe.\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4c1d08-f67d-4f4c-b012-de01b740af0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The no. of unique geolocations_zip_code_prefix has increased by 156 (from Customers) to 19,173\n",
    "geolocations_df6.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1a59e011-094a-4a29-a94c-b8f48f0a5b03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "census_spark_df = spark.createDataFrame(census_df3)\n",
    "\n",
    "# Write Spark DataFrame to delta\n",
    "census_spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/mnt/olist-silver/census\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3d48167f-7104-46aa-aaa6-d75ba5e27a78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "customers_spark_df = spark.createDataFrame(customers_df4)\n",
    "\n",
    "# Write Spark DataFrame to CSV\n",
    "customers_spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/mnt/olist-silver/customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "47103779-d03b-47c2-9878-454c4a45ea4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "sellers_spark_df = spark.createDataFrame(sellers_df4)\n",
    "\n",
    "# Write Spark DataFrame to delta\n",
    "sellers_spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/mnt/olist-silver/sellers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b502b416-6b18-4d52-9b3d-86e65f19e7e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "geolocations_spark_df = spark.createDataFrame(geolocations_df6)\n",
    "\n",
    "# Write Spark DataFrame to delta\n",
    "geolocations_spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/mnt/olist-silver/geolocations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8141ccd2-3b80-4ce5-8ea0-6c9466a64b50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transform Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "286b1ad7-20d7-4c31-b396-206442eb98e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1 Convert relevent spark dataframes to pandas dataframe and EDA\n",
    "\n",
    "reviews_df1 = pd.DataFrame(spark.read.format(\"parquet\").load(input_path_reviews,header=True).collect())\n",
    "\n",
    "# Check data quantity\n",
    "print(reviews_df1.shape)\n",
    "# Display first few rows\n",
    "print(reviews_df1.head())\n",
    "#Data types of each column and the number of non-null values.\n",
    "print(reviews_df1.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "242e216c-8b55-4b1e-b2f4-a97d1f3dd537",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Correct column names, data types and standardize format.\n",
    "\n",
    "columns = {0: \"review_id\", 1: \"order_id\", 2:\"review_score\",3: \"review_comment_title\", 4: \"review_comment_message\", 5: \"review_creation_date\",6:\"review_answer_timestamp\"}\n",
    "reviews_df1.rename(columns=columns, inplace=True)\n",
    "\n",
    "# Custom function to parse dates with flexible format handling\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except ValueError:\n",
    "        return pd.NaT\n",
    "\n",
    "# Convert the data type of the 'review_id','order_id','review_comment_title','review_comment_message' columns to string\n",
    "reviews_df1[['review_id','order_id','review_comment_title','review_comment_message']] = reviews_df1[['review_id','order_id','review_comment_title','review_comment_message']].astype('string')\n",
    "\n",
    "# Convert the 'review_creation_date','review_answer_timestamp' columns to datetime format using custom parser\n",
    "reviews_df1[['review_creation_date', 'review_answer_timestamp']] = reviews_df1[\n",
    "    ['review_creation_date', 'review_answer_timestamp']\n",
    "].applymap(parse_date)\n",
    "\n",
    "reviews_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "222db4b6-7ec0-4386-adbd-bf5b40c885d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Fill empty rows with Null and drop duplicate reviews\n",
    "\n",
    "# Fill empty rows in 'review_comment_title' column with 'NULL'\n",
    "reviews_df1['review_comment_title'] = reviews_df1['review_comment_title'].fillna('NULL')\n",
    "reviews_df1['review_comment_message'] = reviews_df1['review_comment_message'].fillna('NULL')\n",
    "\n",
    "# Replace 'NaN' with 'NULL' in the 'review_comment_title' column\n",
    "reviews_df1['review_comment_title'] = reviews_df1['review_comment_title'].replace('nan', 'NULL')\n",
    "reviews_df1['review_comment_message'] = reviews_df1['review_comment_message'].replace('nan', 'NULL')\n",
    "\n",
    "# Keep only the rows where 'review_id' is unique\n",
    "reviews_df1.drop_duplicates(subset=['review_id'], inplace=True)\n",
    "\n",
    "%store reviews_df1\n",
    "reviews_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9177b2-deb6-4082-896b-b21a061f3ff0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Further cleaning of review_comment_message column to form reviews_text\n",
    "\n",
    "# Create a new DataFrame named reviews_text as a copy of reviews_full\n",
    "reviews_text= reviews_df1.copy()\n",
    "\n",
    "# Filter out rows where review_comment_message is blank\n",
    "reviews_text = reviews_text[reviews_text['review_comment_message'].notnull()]\n",
    "\n",
    "# Filter out rows where review_comment_message contains only non-alphabetic characters\n",
    "reviews_text = reviews_text[reviews_text['review_comment_message'].str.replace(r'[^a-zA-Z]', '', regex=True).str.len() > 0]\n",
    "\n",
    "# Define a function to count the number of words in a string\n",
    "def count_words(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "# Apply the function to count the number of words in each review_comment_message\n",
    "reviews_text['num_words'] = reviews_text['review_comment_message'].apply(count_words)\n",
    "\n",
    "# Filter out rows where the number of words is less than or equal to 2\n",
    "reviews_text = reviews_text[reviews_text['num_words'] > 2]\n",
    "\n",
    "# Drop the 'num_words' column as it's no longer needed\n",
    "reviews_text = reviews_text.drop(columns=['num_words'])\n",
    "\n",
    "# Randomly sample 1000 rows from reviews_text\n",
    "reviews_text = reviews_text.sample(n=1000, random_state=42)\n",
    "\n",
    "# Display the shape of new DataFrame\n",
    "print(reviews_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182014d7-c940-499a-9cad-d6474125aa76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "order_reviews_spark_full = spark.createDataFrame(reviews_df1)\n",
    "order_reviews_spark_text = spark.createDataFrame(reviews_text)\n",
    "\n",
    "# Write Spark DataFrame to delta\n",
    "order_reviews_spark_full.write.format(\"delta\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/mnt/olist-silver/order_reviews_full\")\n",
    "order_reviews_spark_text.write.format(\"delta\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/mnt/olist-silver/order_reviews_fortrans\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "olist-silver-transform",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
